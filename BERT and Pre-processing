import numpy as np
import torch
import transformers

#Initialize the tokernizer as an instance of BertTokenizer() with the name of the pre-trained model
tokenizer = transformers.BertTokenizer.from_pretrained('ber-base-uncased')


#Convert the text into IDs of tokens, and the BERT tokenizer will return IDs of tokens rather than tokens
example = "It is very handy to use transformers"
ids = tokenizer.encode(example, add_special_tokens = True, max_length = 5, truncation = True)

print(ids)


#The token IDs that are output are essentially numerical indices for tokens in the internal dictionary used by BERT


#To operate the model correctly, we set the add_special_tokens argument to True. It means that we added the beginning token and the end token to any text that's being transformed
n = 512

padded = np.array(ids[:n] + [0]*(n - len(ids)))


#Now we have to tell the model why zeros don't carry significant information also known as attention
attention_mask = np.where(padded != 0, 1, 0)

print(attention_mask.shape)






# Example 2
# Initialization
import loggin
import numpy as np
import pandas as pd
import torch
import transformers

# Load Data
data = pd.read_csv('/datasets/imdb_reviews_small.tsv', sep = '\t')

# BERT Tokenizer
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

text = 'It is very handy to use transformers'

logging.getLogger("transformers.tokenization_utils").setLevel(logging.ERROR)

ids = tokenizer.encode(text.lower(), add_special_tokens = True)

n = 512
padded np.array(ids[:n] + [0]*(n - len(ids)))

attention_mask = np.where(padded != 0, 1, 0)

print(ids)

print(padded)

print(attention_mask)


def tokenize_with_bert(texts):

  ids_list = []
  attention_mask_list = []

  min_tokenized_text_length = 1e7
  max_tokenized_text_length = 0

  print(f'The minimum length of vectors: {min_tokenized_text_length}')
  print(f'The maximum length of vectors: {max_tokenized_text_length}')


ids_list, attention_mask_list = tokenize_with_bert(texts = data['review'])
